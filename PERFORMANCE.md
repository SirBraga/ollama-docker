# ‚ö° Guia de Otimiza√ß√£o de Performance - Ollama

## üöÄ Otimiza√ß√µes Aplicadas

### 1. **Docker Compose Otimizado**

```yaml
environment:
  - OLLAMA_NUM_PARALLEL=2              # Processa 2 requisi√ß√µes simultaneamente
  - OLLAMA_MAX_LOADED_MODELS=1         # Mant√©m apenas 1 modelo na RAM
  - OLLAMA_FLASH_ATTENTION=1           # Ativa Flash Attention (30-50% mais r√°pido)
  - OLLAMA_NUM_GPU=0                   # For√ßa CPU
  - OLLAMA_NUM_THREAD=4                # 4 threads da CPU
  - OLLAMA_KEEP_ALIVE=5m               # Mant√©m modelo carregado por 5min

mem_limit: 6g                          # Limite de RAM
memswap_limit: 6g                      # Desabilita swap (evita lentid√£o)
cpu_count: 4                           # Usa 4 CPUs
shm_size: 2gb                          # Mem√≥ria compartilhada
```

### 2. **O que cada otimiza√ß√£o faz:**

| Configura√ß√£o | O que faz | Impacto |
|--------------|-----------|---------|
| `OLLAMA_NUM_PARALLEL=2` | Processa 2 requisi√ß√µes ao mesmo tempo | ‚ö°‚ö° Reduz fila |
| `OLLAMA_MAX_LOADED_MODELS=1` | Mant√©m s√≥ 1 modelo na RAM | üíæ Economiza mem√≥ria |
| `OLLAMA_FLASH_ATTENTION=1` | Algoritmo mais r√°pido de aten√ß√£o | ‚ö°‚ö°‚ö° 30-50% mais r√°pido |
| `OLLAMA_NUM_THREAD=4` | Usa 4 threads da CPU | ‚ö°‚ö° Usa CPU melhor |
| `OLLAMA_KEEP_ALIVE=5m` | N√£o descarrega modelo por 5min | ‚ö°‚ö°‚ö° Evita reload |
| `mem_limit=6g` | Limita RAM a 6GB | üõ°Ô∏è Evita OOM kill |
| `memswap_limit=6g` | Desabilita swap | ‚ö°‚ö°‚ö° Evita disco lento |
| `shm_size=2gb` | Mem√≥ria compartilhada | ‚ö° Comunica√ß√£o r√°pida |

## üìä Ganhos de Performance Esperados

### Antes das otimiza√ß√µes:
```
gemma2:2b ‚Üí 10-15s
phi3 ‚Üí 25-30s
llama3.2 ‚Üí 40s+
```

### Depois das otimiza√ß√µes:
```
gemma2:2b ‚Üí 5-8s ‚ö° (40-50% mais r√°pido)
phi3 ‚Üí 15-20s ‚ö° (30-40% mais r√°pido)
llama3.2 ‚Üí 25-30s ‚ö° (25-35% mais r√°pido)
```

## üîß Como Aplicar

### 1. Atualizar configura√ß√µes:

```bash
cd /Users/sirbraga/Documents/ollama-docker

# Copiar .env.example se n√£o tiver .env
cp .env.example .env

# Editar .env e ajustar OLLAMA_THREADS conforme sua VPS
nano .env
```

### 2. Ajustar threads conforme sua VPS:

```env
# VPS com 2 vCPUs
OLLAMA_THREADS=2

# VPS com 4 vCPUs (recomendado)
OLLAMA_THREADS=4

# VPS com 8 vCPUs
OLLAMA_THREADS=6
```

### 3. Reiniciar containers:

```bash
docker-compose down
docker-compose up -d
```

### 4. Verificar logs:

```bash
docker logs -f ollama
```

## üéØ Modelos Recomendados por Performance

### Para VPS com 7.7GB RAM:

| Modelo | RAM Usada | Velocidade | Qualidade | Recomenda√ß√£o |
|--------|-----------|------------|-----------|--------------|
| **gemma2:2b** | ~2GB | ‚ö°‚ö°‚ö°‚ö°‚ö° 5-8s | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ **MELHOR ESCOLHA** |
| **qwen2.5:1.5b** | ~1.5GB | ‚ö°‚ö°‚ö°‚ö°‚ö° 3-5s | ‚≠ê‚≠ê‚≠ê | ‚úÖ Mais r√°pido |
| **phi3:mini** | ~2.5GB | ‚ö°‚ö°‚ö°‚ö° 10-15s | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ Bom equil√≠brio |
| **llama3.2:3b** | ~3GB | ‚ö°‚ö°‚ö° 15-20s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è Mais lento |
| **mistral:7b** | ~5GB | ‚ö°‚ö° 30s+ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå Muito lento |

### Trocar modelo:

```bash
# Baixar novo modelo
docker exec -it ollama ollama pull qwen2.5:1.5b

# Atualizar .env do bot
nano /Users/sirbraga/Documents/api-teste/.env
```

```env
OLLAMA_MODEL=qwen2.5:1.5b
```

## üî• Otimiza√ß√µes Adicionais

### 1. **Usar modelo quantizado menor:**

```bash
# Ao inv√©s de gemma2:2b (Q4), use:
docker exec -it ollama ollama pull gemma2:2b-q2_k

# Ou use modelo ainda menor:
docker exec -it ollama ollama pull qwen2.5:0.5b
```

### 2. **Reduzir `num_predict` no c√≥digo:**

Edite `ollama_ai.js`:

```javascript
options: {
    temperature: 0.8,
    top_p: 0.9,
    num_predict: 128,  // Reduzir de 256 para 128 (respostas mais curtas)
}
```

### 3. **Desabilitar busca web autom√°tica:**

Se a busca web est√° deixando lento, comente no `ollama_ai.js`:

```javascript
// Comentar detec√ß√£o de busca web
// const searchMatch = assistantMessage.match(/\[BUSCAR:\s*(.+?)\]/i);
```

### 4. **Aumentar timeout se necess√°rio:**

```javascript
}, {
    timeout: 90000  // Aumentar de 60s para 90s
});
```

## üìà Monitoramento

### Ver uso de recursos:

```bash
# CPU e RAM do container
docker stats ollama

# Logs em tempo real
docker logs -f ollama

# Modelos carregados
docker exec -it ollama ollama ps
```

### Testar performance:

```bash
# Tempo de resposta
time curl http://localhost:11434/api/generate -d '{
  "model": "gemma2:2b",
  "prompt": "Ol√°, como voc√™ est√°?",
  "stream": false
}'
```

## üõ†Ô∏è Troubleshooting

### Container est√° usando muito swap:

```bash
# Verificar swap
free -h

# Reduzir mem_limit no docker-compose.yml
mem_limit: 4g  # Ao inv√©s de 6g
```

### Respostas ainda lentas:

1. ‚úÖ Trocar para modelo menor (`qwen2.5:1.5b` ou `qwen2.5:0.5b`)
2. ‚úÖ Reduzir `num_predict` para 128 ou 64
3. ‚úÖ Aumentar `OLLAMA_NUM_THREAD` se tiver mais vCPUs
4. ‚úÖ Desabilitar busca web autom√°tica
5. ‚úÖ Usar quantiza√ß√£o Q2 ao inv√©s de Q4

### Container morrendo (OOM):

```bash
# Reduzir mem_limit
mem_limit: 4g

# Ou usar modelo menor
OLLAMA_MODEL=qwen2.5:0.5b
```

## üéØ Configura√ß√£o Ideal para VPS 7.7GB RAM

```env
# .env do Ollama Docker
OLLAMA_MODELS=qwen2.5:1.5b
OLLAMA_THREADS=4
```

```env
# .env do Bot
OLLAMA_MODEL=qwen2.5:1.5b
```

```yaml
# docker-compose.yml
mem_limit: 5g
cpu_count: 4
```

```javascript
// ollama_ai.js
options: {
    temperature: 0.8,
    top_p: 0.9,
    num_predict: 128,  // Respostas curtas
}
```

**Resultado esperado:** 3-5s por resposta ‚ö°‚ö°‚ö°‚ö°‚ö°

## üìù Checklist de Otimiza√ß√£o

- [x] Flash Attention ativado
- [x] Limites de RAM configurados
- [x] Swap desabilitado
- [x] Threads da CPU otimizadas
- [x] Keep alive configurado
- [ ] Modelo menor instalado (qwen2.5:1.5b)
- [ ] num_predict reduzido (128)
- [ ] Testes de performance realizados

## üöÄ Pr√≥ximos Passos

1. Reiniciar containers com novas configura√ß√µes
2. Testar performance com `!ollama teste`
3. Se ainda lento, trocar para `qwen2.5:1.5b`
4. Monitorar com `docker stats ollama`
5. Ajustar conforme necess√°rio
