# ğŸ¦™ Ollama Docker - LLaMA na VPS

Deploy do Ollama com LLaMA usando Docker para sua VPS.

## ğŸ“‹ PrÃ©-requisitos

- Docker e Docker Compose instalados
- MÃ­nimo 8GB RAM (recomendado 16GB+ para modelos maiores)
- GPU NVIDIA (opcional, mas recomendado para performance)

## ğŸš€ Quick Start

### 1. Clone/Copie para sua VPS

```bash
scp -r ollama-docker user@sua-vps:/home/user/
```

### 2. Execute o setup

```bash
cd ollama-docker
chmod +x scripts/*.sh
./scripts/setup.sh
```

O script detecta automaticamente se hÃ¡ GPU disponÃ­vel.

## ğŸ“ Estrutura

```
ollama-docker/
â”œâ”€â”€ docker-compose.yml      # Config com GPU NVIDIA
â”œâ”€â”€ docker-compose.cpu.yml  # Config CPU only
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup.sh           # Setup inicial
â”‚   â”œâ”€â”€ start.sh           # Iniciar containers
â”‚   â”œâ”€â”€ stop.sh            # Parar containers
â”‚   â”œâ”€â”€ pull-model.sh      # Baixar modelos
â”‚   â””â”€â”€ logs.sh            # Ver logs
â””â”€â”€ README.md
```

## ğŸ¯ Endpoints

| ServiÃ§o | URL | DescriÃ§Ã£o |
|---------|-----|-----------|
| Ollama API | `http://localhost:11434` | API REST do Ollama |
| Open WebUI | `http://localhost:3000` | Interface web (ChatGPT-like) |

## ğŸ’» Comandos Ãšteis

### Gerenciar Containers

```bash
# Iniciar
./scripts/start.sh

# Parar
./scripts/stop.sh

# Ver logs
./scripts/logs.sh
```

### Gerenciar Modelos

```bash
# Listar modelos instalados
docker exec ollama ollama list

# Baixar novo modelo
./scripts/pull-model.sh llama3.2
./scripts/pull-model.sh codellama
./scripts/pull-model.sh mistral

# Chat direto no terminal
docker exec -it ollama ollama run llama3.2
```

## ğŸ”§ Modelos Recomendados

| Modelo | Tamanho | RAM MÃ­nima | Uso |
|--------|---------|------------|-----|
| `llama3.2` | 2GB | 8GB | Chat geral |
| `llama3.2:1b` | 1.3GB | 4GB | Leve, rÃ¡pido |
| `llama3.1:8b` | 4.7GB | 16GB | Mais capaz |
| `codellama` | 3.8GB | 16GB | CÃ³digo |
| `mistral` | 4.1GB | 16GB | Alternativa |

## ğŸŒ API REST

### Chat Completion

```bash
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    { "role": "user", "content": "OlÃ¡, como vocÃª estÃ¡?" }
  ]
}'
```

### Generate (streaming)

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Explique Docker em 3 frases"
}'
```

### Listar Modelos

```bash
curl http://localhost:11434/api/tags
```

## ğŸ”’ SeguranÃ§a na VPS

### Firewall (UFW)

```bash
# Permitir apenas acesso local ou via reverse proxy
sudo ufw deny 11434
sudo ufw deny 3000

# Ou permitir de IP especÃ­fico
sudo ufw allow from SEU_IP to any port 3000
```

### Nginx Reverse Proxy (opcional)

```nginx
server {
    listen 80;
    server_name ollama.seudominio.com;

    location / {
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }
}
```

## ğŸ› Troubleshooting

### Container nÃ£o inicia com GPU

```bash
# Verifique se nvidia-container-toolkit estÃ¡ instalado
nvidia-smi
docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi

# Se nÃ£o funcionar, use a versÃ£o CPU
docker compose -f docker-compose.cpu.yml up -d
```

### Modelo muito lento

- Verifique RAM disponÃ­vel: `free -h`
- Use modelo menor: `llama3.2:1b`
- Adicione swap se necessÃ¡rio

### Erro de memÃ³ria

```bash
# Aumentar swap
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

## ğŸ“š Recursos

- [Ollama Docs](https://ollama.ai)
- [Ollama Models](https://ollama.ai/library)
- [Open WebUI](https://github.com/open-webui/open-webui)
